apiVersion: galasa.dev/v1alpha1
kind: GalasaEcosystem
metadata:
  # Objects instantiated from the operator will use the name of the galasa ecosystem in the 
  # resource names. This also allows for multiple ecosystems to be brought up in the same namespace.
  name: galasa-ecosystem
spec:
  # Version of galasa to use. Individual components can be overridden.
  galasaVersion: "0.14.0"
  # Its possible to use personal repo's if all the required artifacts are stored there. The basis 
  # for what is required can be found in the maven-prod at https://nexus.galasa.dev
  mavenRepository: "https://nexus.galasa.dev/repository/maven-prod/"
  # The external hostname or IP for the k8's cluster being used
  externalhostname: "HOSTNAME"
  # If you want the bootstrap URL and the Grafana to be ingressed, can can use the nginx-ingress controller
  ingressClass: "nginx-example"
  # The external URL of the ingress endpoint
  ingressHostname: https://hostname.example.com
  # Allow to specify the storage class name, omit if you want to use the default
  storageClassName: className
  
  # General galasa config, please go to galasa.dev for full details.
  config:
    node_arch: "amd64"

  # Engine controller is responsible for bringing up engines that run the tests, it will be monitoring the 
  # queues and active tests to ensure that it only runs what the backing infrastructure can support. 
  # Configuration that can be specified:
  # - Replicas: only one is required but scan be scaled to 0
  # - NodeSelector: pass any labels if you want pods to be running on a specific node
  # - ControllerImageName: if pulling from a different repo, the name can be overriden from the docker.galasa.dev default image
  # - Image Version: override the galasa default version for running different level components
  engineController:
    replicas: 1

  # Resource Management is responsible for maintaining the state of the resources, cleaning up after 
  # any tests, ensuring resources are always prepared for the next test
  # Configuration that can be specified:
  # - Replicas: only one is required but scan be scaled to 0
  # - NodeSelector: pass any labels if you want pods to be running on a specific node
  # - ResourceMonitorImageName: if pulling from a different repo, the name can be overriden from the docker.galasa.dev default image
  # - Image Version: override the galasa default version for running different level components
  engineResmon:
    replicas: 1

  # The API server is the access point for both IDE's and pipelines, for submitting tests, seeing running tests 
  # and for pulling test results.
  # Configuration that can be specified:
  # - Replicas: only one is required but scan be scaled to 0
  # - NodeSelector: pass any labels if you want pods to be running on a specific node
  # - Storage: Size of the PVC the service will ask for
  # - ApiServerImageName: if pulling from a different repo, the name can be overriden from the docker.galasa.dev default image
  # - Image Version: override the galasa default version for running different level components
  apiserver:
    replicas: 1
    storage: "200m"

  # The property store hosts both the CPS (configuration property store) and the DSS (Dynamic status store). It is 
  # a ETCD cluster, but only currently supports cluster sizes of 1. Bigger clusters will be available soon
  # Configuration that can be specified:
  # - Replicas: only 1 is supported but scan be scaled to 0
  # - NodeSelector: pass any labels if you want pods to be running on a specific node
  # - InitProps: properties that will be added to the CPS upon a PVC's instantiation. More details for what properties 
  #   can be set can be fond at https://galasa.dev
  # - Storage: Size of the PVC the service will ask for
  # - PropertyStoreImageName: if pulling from a different repo, the name can be overriden from the dockerhub default
  # - Image Version: override the galasa default version for running different level components
  propertystore:
    clusterSize: 1
    storage: "200m"
    InitProps:
      framework.resource.management.dead.heartbeat.timeout: "30"
      framework.resource.management.finished.timeout: "40"

  # The RAS (results archive store) stores all the test runs, artifacts and logs. Current implementation and this operator 
  # use CouchDB
  # Configuration that can be specified:
  # - Replicas: only 1 is supported but scan be scaled to 0
  # - NodeSelector: pass any labels if you want pods to be running on a specific node
  # - Storage: Size of the PVC the service will ask for
  # - RasImageName: if pulling from a different repo, the name can be overriden from the dockerhub default
  # - Image Version: override the galasa default version for running different level components
  rasSpec:
    replicas: 1
    storage: "200m"

  # Simbank is an application that can simulate an application running on a mainframe and its responses to galasa. Galasa can 
  # not differentiate Simbank from a normal mainframe application. This application can be used to test if the ecosystem is working 
  # correctly. Performing a 'kubectl get GalasaEcosystem', will show you all the ecosystems in the active namespace, whether 
  # the ecosystem is ready, and the bootstrap URL required for your IDE plugin. It also provides the URL to the grafana which can be
  # to monitor the running tests
  # Configuration that can be specified:
    # - SimbankImageName: if pulling from a different repo, the name can be overriden from the docker.galasa.dev default image
    # - Image Version: override the galasa default version for running different level components
  simbank:
    replicas: 1

  # The operator will bring up its own monitoring system if you dont have one. If you do not need a grafana or prometheus, please 
  # scale these to 0. You will likely want to keep the metrics deployment so you can point your own promtheus server towards it.
  monitoring:
    grafanaReplicas: 1
    grafanaStorage: "200m"
    prometheusReplicas: 1
    prometheusStorage: "200m"
    metricsReplicas: 1
    metricsStorage: "200m"

